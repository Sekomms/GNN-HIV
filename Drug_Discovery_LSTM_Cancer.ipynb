{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNHFJ-SCRRnK",
        "outputId": "38cc57b8-57bd-45be-f3a4-067fe2bbb3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit-pypi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY-Rbn-BLjDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99832326-ecf5-4e23-8529-caa4adfe2148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Crippen\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random as rn\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, Descriptors\n",
        "%matplotlib inline\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuBecULiL88D"
      },
      "outputs": [],
      "source": [
        "seed = 23\n",
        "np.random.seed(seed)\n",
        "rn.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5Jz-OKZNYCs"
      },
      "outputs": [],
      "source": [
        "# veriyi dosyadan okuyoruz\n",
        "csv_path = \"/content/dosya_yeni.csv\"\n",
        "\n",
        "# burada geçenki çalışmamızda olan aynı verisetini kullanıyoruz fakat hatırlarsanız\n",
        "# drug discovery için sadece 1 sınıfına ihtiyacımız var 0 sınıfı bizim için gereksiz\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "df['smiles'] = df['smiles'].astype('str')\n",
        "mask = (df['cancer_active']==1)\n",
        "df = df.loc[mask]\n",
        "mask = (df['smiles'].str.len() < 128)\n",
        "df = df.loc[mask]\n",
        "smiles = df['smiles']\n",
        "data = df['smiles']\n",
        "full_train, test = train_test_split(data, test_size=0.2, random_state=seed)\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE1TcvQEN0Ij"
      },
      "outputs": [],
      "source": [
        "val_split = 0.10\n",
        "train, val_set = train_test_split(full_train, test_size=val_split, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRp6rMiwN850"
      },
      "outputs": [],
      "source": [
        "char_to_int = {'n': 0, '[': 1, '\\\\': 2, 'E': 3, 'H': 4, ')': 5, 'B': 6, '9': 7, '2': 8, ']': 9, '7': 10, '!': 11, 't': 12, 's': 13, 'o': 14, 'c': 15, 'K': 16, '-': 17, '/': 18, 'l': 19, 'A': 20, 'r': 21, '@': 22, 'C': 23, '=': 24, '6': 25, 'N': 26, 'L': 27, 'a': 28, '5': 29, 'S': 30, 'T': 31, '#': 32, '+': 33, 'P': 34, 'i': 35, '(': 36, '8': 37, '1': 38, 'I': 39, 'e': 40, 'O': 41, '3': 42, 'F': 43, '4': 44, '.': 45, 'Z': 46, 'b': 47, 'G': 48, 'd': 49, 'g': 50, 'm': 51, 'U': 52, 'u': 53, 'X':54, '$':55, 'R':56, 'h':57, '%':58, '0':59}\n",
        "int_to_char = {'0': 'n', '1': '[', '2': '\\\\', '3': 'E', '4': 'H', '5': ')', '6': 'B', '7': '9', '8': '2', '9': ']', '10': '7', '11': '!', '12': 't', '13': 's', '14': 'o', '15': 'c', '16': 'K', '17': '-', '18': '/', '19': 'l', '20': 'A', '21': 'r', '22': '@', '23': 'C', '24': '=', '25': '6', '26': 'N', '27': 'L', '28': 'a', '29': '5', '30': 'S', '31': 'T', '32': '#', '33': '+', '34': 'P', '35': 'i', '36': '(', '37': '8', '38': '1', '39': 'I', '40': 'e', '41': 'O', '42': '3', '43': 'F', '44': '4', '45': '.', '46': 'Z', '47': 'b', '48': 'G', '49': 'd', '50': 'g', '51': 'm', '52': 'U', '53': 'u', '54':'X', '55':'$', '56':'R', '57':'h', '58':'%', '59':'0'}\n",
        "n_vocab = len(char_to_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-izC820OWLK"
      },
      "outputs": [],
      "source": [
        "# her bir smiles datası için sabit olan sequence boyutuna göre padding yapıyoruz padding için büyük E harfi kullanıyoruz\n",
        "# ayrıca her bir smiles sekansının ! işaretiyle başlamasını sağlıyoruz\n",
        "# bu elde ettiğimiz vektörler ile yapay zekayı besleyeceğiz bu sayede yapay zeka smiles strings içerisindeki gramatic kuralları anlayabilecek\n",
        "def vectorize(smiles, embed, n_vocab):\n",
        "    one_hot = np.zeros((smiles.shape[0], embed, n_vocab), dtype=np.int8)\n",
        "    for i, smile in enumerate(smiles):\n",
        "        one_hot[i,0,char_to_int[\"$\"]] = 1\n",
        "        for j, c in enumerate(smile):\n",
        "            one_hot[i,j+1,char_to_int[c]] = 1\n",
        "        one_hot[i,len(smile)+1:,char_to_int[\"X\"]] = 1\n",
        "    return one_hot[:,0:-1,:], one_hot[:,1:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgPEyqTEOkya"
      },
      "outputs": [],
      "source": [
        "# daha önce train test split işlemini zaten yapmıştık burada split edilmiş veri setini vectorize ediyoruz\n",
        "# yani smiles veriseti tamamen vektörize ediliyor çünkü lstm network vektör ile çalışır\n",
        "embed = 128\n",
        "X_train, y_train = vectorize(train, embed, n_vocab)\n",
        "X_val, y_val = vectorize(val_set, embed, n_vocab)\n",
        "X_test, y_test = vectorize(test, embed, n_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test[0])"
      ],
      "metadata": {
        "id": "yJI5F-qYDE8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMzkMFJOxla"
      },
      "outputs": [],
      "source": [
        "# lstm katmanı\n",
        "# encoder katmanı\n",
        "enc_input = Input(shape=(X_train.shape[1:]))\n",
        "_, state_h, state_c = LSTM(128, return_state=True)(enc_input)\n",
        "states = Concatenate(axis=-1)([state_h, state_c])\n",
        "bottle_neck = Dense(64, activation='relu')(states)\n",
        "\n",
        "# decoder katmanı\n",
        "state_h_decoded = Dense(128, activation='relu')(bottle_neck)\n",
        "state_c_decoded = Dense(128, activation='relu')(bottle_neck)\n",
        "encoder_states = [state_h_decoded, state_c_decoded]\n",
        "dec_input = Input(shape=(X_train.shape[1:]))\n",
        "dec1 = LSTM(128, return_sequences=True)(dec_input, initial_state=encoder_states)\n",
        "output = Dense(y_train.shape[2], activation='softmax')(dec1)\n",
        "\n",
        "model = Model(inputs=[enc_input, dec_input], outputs=output)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_h"
      ],
      "metadata": {
        "id": "zy-uUCY6-a7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgZXUe6FO2fS"
      },
      "outputs": [],
      "source": [
        "# compile\n",
        "batch_size = 16\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPr0aj8dO5H6"
      },
      "outputs": [],
      "source": [
        "# batch öğrenme için data generatörü\n",
        "class Data_Generator(Sequence):\n",
        "    def __init__(self, input_data, labels, batch_size):\n",
        "        self.input_data, self.labels = input_data, labels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.input_data) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.input_data[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        batch_x, batch_y = np.array(x), np.array(y)\n",
        "\n",
        "        return [batch_x, batch_x], batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBshhYJ0O69b"
      },
      "outputs": [],
      "source": [
        "training_generator = Data_Generator(X_train, y_train, batch_size)\n",
        "validation_generator = Data_Generator(X_val, y_val, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI6YtSlGPcqC"
      },
      "outputs": [],
      "source": [
        "# fit the model\n",
        "nb_epochs = 200\n",
        "validation_steps = len(X_val) // batch_size\n",
        "history = model.fit(training_generator, steps_per_epoch=steps_per_epoch, epochs=nb_epochs, verbose=1,\n",
        "                              validation_data=validation_generator, validation_steps=validation_steps,\n",
        "                             use_multiprocessing=False, shuffle=True, callbacks=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFrSmvMePjrC"
      },
      "outputs": [],
      "source": [
        "# eğitilen lstm modelinin accuracy ve loss grafikleri\n",
        "fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n",
        "\n",
        "axis1.plot(history.history[\"acc\"], label='Train', linewidth=3)\n",
        "axis1.plot(history.history[\"val_acc\"], label='Validation', linewidth=3)\n",
        "axis1.set_title('Model accuracy', fontsize=16, color=\"white\")\n",
        "axis1.set_ylabel('accuracy')\n",
        "axis1.set_xlabel('epoch')\n",
        "axis1.legend(loc='lower right')\n",
        "\n",
        "axis2.plot(history.history[\"loss\"], label='Train', linewidth=3)\n",
        "axis2.plot(history.history[\"val_loss\"], label='Validation', linewidth=3)\n",
        "axis2.set_title('Model loss', fontsize=16, color=\"white\")\n",
        "axis2.set_ylabel('loss')\n",
        "axis2.set_xlabel('epoch')\n",
        "axis2.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akgHepMgQZlz"
      },
      "outputs": [],
      "source": [
        "encoder_model = Model(inputs=model.layers[0].input, outputs=model.layers[3].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m_fq3FqQcKj"
      },
      "outputs": [],
      "source": [
        "# encoder modelden gelen hidden state ve cell state değerlerini input olarak alıp sonraki decoder modele input olarak verecek olan model\n",
        "\n",
        "latent_input = Input(shape=(64, ))\n",
        "state_h = model.layers[5](latent_input)\n",
        "state_c = model.layers[6](latent_input)\n",
        "latent_to_states_model = Model(latent_input, [state_h, state_c])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3phrSh3UQeSo"
      },
      "outputs": [],
      "source": [
        "# ilaç üretecek olan kısım burası burada input katmanının 1,1,54 olması\n",
        "# bu modelin çıktı olarak 54 feature'ı olan bir smiles vereceği anlamına gelir\n",
        "decoder_inputs = Input(batch_shape=(1, 1, 60))\n",
        "decoder_lstm = LSTM(128, return_sequences=True, stateful=True)(decoder_inputs)\n",
        "decoder_outputs = Dense(60, activation='softmax')(decoder_lstm)\n",
        "gen_model = Model(decoder_inputs, decoder_outputs)\n",
        "for i in range(1,3):\n",
        "    gen_model.layers[i].set_weights(model.layers[i+6].get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEpctZIzQjkA"
      },
      "outputs": [],
      "source": [
        "# bu fonksiyonda kendi softmax fonskiyonumuzu yazıyoruz bu şekilde bir latent space oluşturabileceğiz\n",
        "def sample_with_temp(preds, sampling_temp):\n",
        "    streched = np.log(preds) / sampling_temp\n",
        "    streched_probs = np.exp(streched) / np.sum(np.exp(streched))\n",
        "    return np.random.choice(range(len(streched)), p=streched_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrY8LXxUQoFy"
      },
      "outputs": [],
      "source": [
        "# Oluşan latent space'den sample smiles değerleri almak için fonksiyon\n",
        "# oluşan latent space'i veriyoruz feature saıyısını veriyouz ve aktivasyon fonksiyonunun parametresini veriyoruz\n",
        "def sample_smiles(latent, n_vocab, sampling_temp):\n",
        "    states = latent_to_states_model.predict(latent)\n",
        "    gen_model.layers[1].reset_states(states=[states[0], states[1]])\n",
        "\n",
        "    startidx = char_to_int[\"$\"]\n",
        "    samplevec = np.zeros((1,1,n_vocab))\n",
        "    samplevec[0,0,startidx] = 1\n",
        "    sequence = \"\"\n",
        "\n",
        "    for i in range(101):\n",
        "        preds = gen_model.predict(samplevec)[0][-1]\n",
        "        if sampling_temp == 1.0:\n",
        "          sampleidx = np.argmax(preds)\n",
        "        else:\n",
        "          sampleidx = sample_with_temp(preds, sampling_temp)\n",
        "        samplechar = int_to_char[str(sampleidx)]\n",
        "        if samplechar != \"X\":\n",
        "            sequence += samplechar\n",
        "            samplevec = np.zeros((1,1,n_vocab))\n",
        "            samplevec[0,0,sampleidx] = 1\n",
        "        else:\n",
        "            break\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkqelkVHQ2iy"
      },
      "outputs": [],
      "source": [
        "# latent space etrafında smiles generationu yapıyoruz\n",
        "def generate(latent_seed, sampling_temp, scale, quant):\n",
        "  samples, mols = [], []\n",
        "  for i in range(quant):\n",
        "    latent_vec = latent_seed + scale*(np.random.randn(latent_seed.shape[1]))\n",
        "    out = sample_smiles(latent_vec, n_vocab, sampling_temp)\n",
        "    mol = Chem.MolFromSmiles(out)\n",
        "    if mol:\n",
        "      try:\n",
        "        hDonorNum = Chem.Lipinski.NumHDonors(mol)\n",
        "        hAcceptorNum = Chem.Lipinski.NumHAcceptors(mol)\n",
        "        logp = Descriptors.MolLogP(mol)\n",
        "        mol_weight = Descriptors.MolWt(mol)\n",
        "        print(f'donor: {hDonorNum}, acceptor: {hAcceptorNum}, logp: {logp}, molweight: {mol_weight}')\n",
        "        if hDonorNum <=5 and hAcceptorNum <= 10 and mol_weight <500 and logp <=5:\n",
        "          mols.append(mol)\n",
        "          samples.append(out)\n",
        "      except:\n",
        "        print('başaramadık')\n",
        "  return mols, samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDdn6uiPQ45i"
      },
      "outputs": [],
      "source": [
        "latent_space = encoder_model.predict(X_train)\n",
        "latent_seed = latent_space[50:51]\n",
        "sampling_temp = 0.75\n",
        "scale = 0.5\n",
        "quantity = 300\n",
        "t_mols, t_smiles = generate(latent_seed, sampling_temp, scale, 300)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Draw.MolsToGridImage(t_mols, molsPerRow=3, subImgSize=(400, 400))"
      ],
      "metadata": {
        "id": "z4n6Psk5YFus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Draw.MolsToGridImage([t_mols[0]], molsPerRow=1, subImgSize=(300, 200))"
      ],
      "metadata": {
        "id": "SItNXdebIVs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdd9VUzujsyy"
      },
      "outputs": [],
      "source": [
        "t_smiles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_lipinski_rule(smiles):\n",
        "    # SMILES dizgisinden molekül oluştur\n",
        "    molecule = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Lipinski'nin 5 kuralını kontrol et\n",
        "    molecular_weight = Chem.rdMolDescriptors.CalcExactMolWt(molecule)\n",
        "    num_heteroatoms = sum(1 for atom in molecule.GetAtoms() if atom.GetSymbol() not in ['C', 'H'])\n",
        "    num_hb_acceptors = Chem.rdMolDescriptors.CalcNumLipinskiHBA(molecule)\n",
        "    num_hb_donors = Chem.rdMolDescriptors.CalcNumLipinskiHBD(molecule)\n",
        "    rotatable_bonds = Chem.rdMolDescriptors.CalcNumRotatableBonds(molecule)\n",
        "    log_p = Crippen.MolLogP(molecule)\n",
        "\n",
        "    # Tüm Lipinski kurallarına uygunsa, SMILES dizgisini ve molekül ağırlığını ekrana yazdır\n",
        "    if (molecular_weight <= 500 and\n",
        "        num_heteroatoms <= 5 and\n",
        "        num_hb_acceptors <= 10 and\n",
        "        num_hb_donors <= 5 and\n",
        "        rotatable_bonds <= 10 and\n",
        "        log_p <= 5):\n",
        "        print(\"SMILES: \", smiles)\n",
        "        print(\"Molekül Ağırlığı: \", molecular_weight)\n"
      ],
      "metadata": {
        "id": "obz421Hkz7OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for smiles in t_smiles:\n",
        "  check_lipinski_rule(smiles)"
      ],
      "metadata": {
        "id": "tbtFr9XY0B5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}